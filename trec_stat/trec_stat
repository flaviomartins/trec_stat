#!/usr/bin/env python
from __future__ import print_function

import logging
import sys
from optparse import OptionParser

import numpy as np
import pandas as pd
import statsmodels.stats.api as sms
from scipy import stats

# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(message)s')

# parse commandline arguments
op = OptionParser()
op.add_option("-m",
              dest="metric", type="str", default="map",
              help="Specify the evaluation metric to use for the test.")
op.add_option("--level", type=float, default=.05,
              help="Significance level.")
op.add_option("--margin", type=float, default=.01,
              help="Equivalence margin.")
op.add_option("--quiet",
              action="store_true", dest="quiet", default=False,
              help="Just output the p-value.")
op.add_option("--verbose",
              action="store_true", dest="verbose", default=False,
              help="Print reports inside tests.")


def is_interactive():
    return not hasattr(sys.modules['__main__'], '__file__')

# work-around for Jupyter notebook and IPython console
argv = [] if is_interactive() else sys.argv[1:]
(opts, args) = op.parse_args(argv)
if len(args) > 2:
    print(__doc__)
    op.print_help()
    op.error("this script takes exactly two arguments.")
    sys.exit(1)

system_a = args[0]
system_b = args[1]
metric = opts.metric
mep = opts.level
delta = opts.margin


tnames = ['measure', 'topic', 'value']
dfA = pd.read_table(system_a, delim_whitespace=True, header=None, names=tnames, engine='python', skipfooter=1)
dfB = pd.read_table(system_b, delim_whitespace=True, header=None, names=tnames, engine='python', skipfooter=1)

dfA = dfA.pivot_table(values='value', index='topic', columns='measure', fill_value='', aggfunc=lambda x: x)
dfB = dfB.pivot_table(values='value', index='topic', columns='measure', fill_value='', aggfunc=lambda x: x)

# drop the row with 'all' results
dfA.drop(['all'], inplace=True)
dfB.drop(['all'], inplace=True)

try:
    # drop runid column
    dfA.drop('runid', axis=1, inplace=True)
    dfB.drop('runid', axis=1, inplace=True)
except (ValueError, KeyError) as e:
    pass

df = dfA[[metric]].copy()
df.columns = ['A']
df['B'] = dfB[[metric]]

# pd.to_numeric() pandas 0.17 alternative
df[['A', 'B']] = df[['A', 'B']].astype(float)

valuesA = df['A'].values
valuesB = df['B'].values


def difference(valuesA, valuesB):
    d = valuesB - valuesA
    dbar = np.mean(d)
    V = np.mean(np.power(d - dbar, 2))
    dd = sms.DescrStatsW(d)

    n = len(valuesA)
    nm1 = n - 1

    espairedt = dbar / np.sqrt(V)

    alpha = 1 - mep
    tinv = stats.t.ppf(1 - (mep / 2), nm1)
    # Sakai's paper uses the formula for large samples 
    # scale = np.sqrt(V / n)
    # However the following should be used for n <= 1000 samples
    scale = np.sqrt(V / nm1)
    me = tinv * scale

    lower = dbar-me
    upper = dbar+me
    # print(lower, upper)

    # lower, upper = dd.tconfint_mean(alpha=mep, alternative='two-sided')
    # print(lower, upper)

    # lower, upper = stats.t.interval(alpha=alpha, df=nm1, loc=dbar, scale=stats.sem(d, ddof=1))
    # print(lower, upper)

    # m_res, v_res, s_res = stats.bayes_mvs(d, alpha=alpha)
    # lower, upper = m_res.minmax
    # print(lower, upper)

    # paired t-test
    # e.g., before and after a treatment
    # stats.ttest_rel(post, pre)
    t, p, dof = dd.ttest_mean(0, alternative='two-sided')
    print('t-test for the difference in {0} mean(d) = {1:.4f} ({2:.2f}\%)'.format(metric, dbar, dbar / np.mean(valuesA) * 100.0))
    print('t({0}) = {1:.2f} and p < {2:f}'.format(nm1, t, p))

    if not opts.quiet:
        if p < 0.05 and t > 0:
            print("According to a two-sided paired t-test for the difference in {10:s} means \(\mean{{d}} = {0:.4f}\) "
                "(with the unbiased estimate of the population variance \(V = {1:.4f}\)), "
                "{8:s} statistically significantly outperforms {9:s} "
                "\((t({2}) = {3:.2f}\), "
                "\(p < {4:f}\), "
                "\(ES_{{pairedt}} = {5:.2f}\), "
                "95\% CI \([{6:.4f},{7:.4f}]\)).".format(dbar, V, nm1, t, p, espairedt, lower, upper, system_b, system_a, metric))

    print('>> ', end='')
    if p < 0.01 and t > 0:
        print('0.01')
        return True
    elif p < 0.05 and t > 0:
        print('0.05')
        return True
    else:
        print('NOT')
        return False


def non_inferiority(valuesA, valuesB):
    d = valuesB - valuesA
    dbar = np.mean(d)
    V = np.mean(np.power(d - dbar, 2))
    dd = sms.DescrStatsW(d)

    n = len(valuesA)
    nm1 = n - 1

    espairedt = dbar / np.sqrt(V)

    alpha = 1 - 2*mep
    tinv = stats.t.ppf(1 - 2*mep, nm1)
    # Sakai's paper uses the formula for large samples 
    # scale = np.sqrt(V / n)
    # However the following should be used for n <= 1000 samples
    scale = np.sqrt(V / nm1)
    me = tinv * scale

    lower = dbar-me
    upper = dbar+me
    # print(lower, upper)

    # paired t-test (non-inferiority)
    # e.g., before and after a treatment
    t, p, dof = dd.ttest_mean(-delta, alternative='larger')
    print('\nt-test for non-inferiority of {0} mean(d) = {1:.4f} ({2:.2f}\%)'.format(metric, dbar, dbar / np.mean(valuesA) * 100.0))
    print('t({0}) = {1:.2f} and p < {2:f}'.format(nm1, t, p))

    if not opts.quiet:
        if lower > -delta and p < 0.05:
            print("According to a one-sided paired t-test for equality or superiority of {10:s} means \(\mean{{d}} = {0:.4f}\) "
                "(with the unbiased estimate of the population variance \(V = {1:.4f}\)), "
                "with equivalence margin \(\delta = {11:.2f}\) "
                "{8:s} is statistically equal or superior to {9:s} "
                "\((t({2}) = {3:.2f}\), "
                "\(p < {4:f}\), "
                "\(ES_{{pairedt}} = {5:.2f}\), "
                "90\% CI \([{6:.4f},{7:.4f}]\)).".format(dbar, V, nm1, t, p, espairedt, lower, upper, system_b, system_a, metric, delta))

    print('>> ', end='')
    if lower > -delta and p < 0.01:
        print('0.01')
        return True
    elif lower > -delta and p < 0.05:
        print('0.05')
        return True
    else:
        print('NOT')
        return False


def equivalence(valuesA, valuesB):
    d = valuesB - valuesA
    dbar = np.mean(d)
    V = np.mean(np.power(d - dbar, 2))
    dd = sms.DescrStatsW(d)

    n = len(valuesA)
    nm1 = n - 1

    espairedt = d / np.sqrt(V)

    alpha = 1 - 2*mep
    tinv = stats.t.ppf(1 - 2*mep, nm1)
    # Sakai's paper uses the formula for large samples 
    # scale = np.sqrt(V / n)
    # However the following should be used for n <= 1000 samples
    scale = np.sqrt(V / nm1)
    me = tinv * scale

    lower = dbar-me
    upper = dbar+me
    # print(lower, upper)

    # paired t-tost (equivalence)
    # e.g., before and after a treatment
    # sms.ttost_paired(post, pre, -delta, delta)
    p, t1, t2 = dd.ttost_mean(-delta, delta)
    if t1[1] < t2[1]:
        t = t2[0]
    else:
        t = t1[0]
    print('\ntwo one-sided t-tests for the equivalence in {0} mean(d) = {1:.4f} ({2:.2f}\%)'.format(metric, dbar, dbar / np.mean(valuesA) * 100.0))
    print('t({0}) = {1:.2f} and p < {2:f}'.format(nm1, t, p))

    if not opts.quiet:
        if lower > -delta and upper < delta and p < 0.05:
            print("According to two one-sided paired t-tests for the equivalence in {10:s} means \(\mean{{d}} = {0:.4f}\) "
                "(with the unbiased estimate of the population variance \(V = {1:.4f}\)), "
                "with equivalence margin \(\delta = {11:.2f}\) "
                "{8:s} is statistically equivalent to {9:s} "
                "\((t({2}) = {3:.2f}\), "
                "\(p < {4:f}\), "
                "\(ES_{{pairedt}} = {5:.2f}\), "
                "90\% CI \([{6:.4f},{7:.4f}]\)).".format(dbar, V, nm1, t, p, espairedt, lower, upper, system_b, system_a, metric, delta))

    print('>> ', end='')
    if lower > -delta and upper < delta and p < 0.01:
        print('0.01')
        return True
    elif lower > -delta and upper < delta and p < 0.05:
        print('0.05')
        return True
    else:
        print('NOT')
        return False


if not difference(valuesA, valuesB):
    if not non_inferiority(valuesA, valuesB):
        equivalence(valuesA, valuesB)
